# -*- coding: utf-8 -*-
"""NLP-POS tagging using HMMs and Viterbi heuristic.ipynb
 Automatically generated by Colaboratory.

 ## NLP-POS tagging using HMMs and Viterbi heuristic

 This project uses the tagged treebank corpus available as a part of the NLTK pack
 age to build a POS tagging algorithm using HMMs and Viterbi heuristic.

 ## 1. Exploring Treebank Tagged Corpus
 """

# Importing libraries
import nltk
import re
import pprint
import numpy as np
import pandas as pd
import requests
import matplotlib.pyplot as plt
import seaborn as sns
import pprint
import time
import random
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize

from kivymd.app import MDApp
from kivymd.uix.screen import Screen
from kivymd.uix.button import MDRaisedButton
from kivy.uix.boxlayout import BoxLayout
from kivymd.uix.label import MDLabel
from kivy.lang.builder import Builder
import helpers
# nltk.download('punkt')

# reading the Treebank tagged sentences
# nltk.download('treebank')
tagged = list(nltk.corpus.treebank.tagged_sents())

# Splitting into train and test
random.seed(1234)
train_set, test_set = train_test_split(tagged, test_size=0.3)

# Getting list of tagged words
train_tagged_words = [tup for sent in train_set for tup in sent]

# tokens
tokens = [pair[0] for pair in train_tagged_words]

# vocabulary
voc = set(tokens)

# number of tags
Tags = set([pair[1] for pair in train_tagged_words])

"""## 2. POS Tagging Algorithm - HMM

 Hidden Markov Model based algorithm is used to tag the words. Given a sequence of
 words to be tagged, the task is to assign the most probable tag to the word.

 In other words, to every word w, assign the tag t that maximises the likelihood P
 (t/w). Since P(t/w) = P(w/t). P(t) / P(w), after ignoring P(w), we have to comput
 e P(w/t) and P(t).

 P(w/t) is basically the probability that given a tag (say NN), what is the probab
 ility of it being w (say 'building'). This can be computed by computing the fract
 ion of all NNs which are equal to w, i.e.

 P(w/t) = count(w, t) / count(t).

 The term P(t) is the probability of tag t, and in a tagging task, we assume that
 a tag will depend only on the previous tag. In other words, the probability of a
 tag being NN will depend only on the previous tag t(n-1). So for e.g. if t(n73 1) is a JJ, then t(n) is likely to be an NN since adjectives often precede a noun
 (blue coat, tall building etc.).

 Given the penn treebank tagged dataset, we can compute the two terms P(w/t) and P
(t) and store them in two large matrices. The matrix of P(w/t) will be sparse, si
 nce each word will not be seen with most tags ever, and those terms will thus be
 zero.

 ### Emission Probabilities (i.e. probability of a word given a tag)
 """

# computing P(w/t) and storing in T x V matrix
t = len(Tags)
v = len(voc)
w_given_t = np.zeros((t, v))

# compute word given tag: Emission Probability


def word_given_tag(word, tag, train_pot=train_tagged_words):
    tag_list = [pair for pair in train_pot if pair[1] == tag]


count_tag = len(tag_list)
w_given_tag_list = [pair[0] for pair in tag_list if pair[0] == word]
count_w_given_tag = len(w_given_tag_list)

return (count_w_given_tag, count_tag)

"""### Transition Probabilities (i.e. probability of getting a tag t2 given that
 the tag for previous word was t1)"""

compute tag given tag: tag2(t2) given tag1(t1), i.e. Transition Probability


def t2_given_t1(t2, t1, train_pot=train_tagged_words):
    tags = [pair[1] for pair in train_pot]
    count_t1 = len([t for t in tags if t == t1])
    count_t2_given_t1 = 0
    for index in range(len(tags)-1):
    if tags[index] == t1 and tags[index+1] == t2:
    count_t2_given_t1 += 1
    return (count_t2_given_t1, count_t1)

  # creating t x t transition matrix of tags
  # each column is t2, each row is t1
  # thus M(i, j) represents P(tj given ti)

    tag_grid = np.zeros((len(Tags), len(Tags)), dtype='float32')


for i, t1 in enumerate(list(Tags)):
    for j, t2 in enumerate(list(Tags)):
    tag_grid[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]

  # convert the matrix to a df for better readability
    tags_dataframe = pd.DataFrame(
        tag_grid, columns=list(Tags), index=list(Tags))

  # heatmap of tags matrix
  # T(i, j) means P(tag j given tag i)
  # plt.figure(figsize=(18, 12))
  # sns.heatmap(tags_dataframe)
  # plt.show()

  # frequent tags
  # filter the df to get P(t2, t1) > 0.5
    frequent_tags = tags_dataframe[tags_dataframe > 0.5]
  # plt.figure(figsize=(18, 12))
  # sns.heatmap(frequent_tags)
  # plt.show()

    """## 3. Viterbi Algorithm

 Let's now use the computed probabilities P(w, tag) and P(t2, t1) to assign tags t
 o each word in the document. We'll run through each word w and compute P(tag/w)=P
 (w/tag).P(tag) for each tag in the tag set, and then assign the tag having the ma
 x P(tag/w).

 We'll store the assigned tags in a list of tuples, similar to the list 'train_tag
 ged_words'. Each tuple will be a (token, assigned_tag). As we progress further in
 the list, each tag to be assigned will use the tag of the previous token.

 Note: P(tag|start) = P(tag|'.')
 """

  # Viterbi Heuristic
    def Viterbi(words, train_pot=train_tagged_words):
    state = []
Tags = list(set([pair[1] for pair in train_pot]))
for key, word in enumerate(words):
    # initialise list of probability column for a given observation
    p = []
    for tag in Tags:
    if key == 0:
    transition_p = tags_dataframe.loc['.', tag]
    else:
    transition_p = tags_dataframe.loc[state[-1], tag]

  # compute emission and state probabilities
    emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[
        168 key], tag)[1]
    state_probability = emission_p * transition_p
    p.append(state_probability)

    pmax = max(p)
  # getting state for which probability is maximum
    state_max = Tags[p.index(pmax)]
    state.append(state_max)
    return list(zip(words, state))

    """## 4. Evaluating on Test Set"""

  # Running the Viterbi algorithm on a few sample sentences
  # since running it on the entire data set will take many hours

    random.seed(1234)

  # choose random 5 sents
    rndom = [random.randint(1, len(test_set)) for x in range(5)]

  # list of sents
    test_run = [test_set[i] for i in rndom]
  # list of tagged words
    test_run_base = [tup for sent in test_run for tup in sent]

  # list of untagged words
    test_tagged_words = [tup[0] for sent in test_run for tup in sent]

  # tagging the test sentences
    start = time.time()
    tagged_seq = Viterbi(test_tagged_words)
    end = time.time()
    difference = end-start

  # accuracy
    check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]

    accuracy = len(check)/len(tagged_seq)

    incorrect_tagged_cases = [[test_run_base[i209 1], j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0] != j[1]]

  # print(tagged_seq)
  # print(difference)

    class MainApp(MDApp):
    def build(self):
    self.theme_cls.primary_palette = "Green"
    self.theme_cls.accent_palette = "Blue"
    self.theme_cls.theme_style = "Dark"
    layout = BoxLayout(orientation='vertical')
    label = MDLabel(text="Tokenizer", halign="center", font_size="64", size_h
                    221 int_y=None, padding_y="15", pos_hint={'center_x': 0.5, 'center_y': 0.9})
    self.tinput = Builder.load_string(helpers.t_input)
    button = MDRaisedButton(text='Tokenize',
                            224 pos_hint={'center_x': 0.5, 'center_y': 0.1
                                          },
                            on_release=self.HMM_Tokens)
    self.toutput = Builder.load_string(helpers.t_output)
    layout.add_widget(label)
    layout.add_widget(self.tinput)
    layout.add_widget(button)
    layout.add_widget(self.toutput)
    return layout


def HMM_Tokens(self, obj):
    # HMM_Tagger.sentence_test = self.tinput.text
    words = word_tokenize(self.tinput.text)
    tagged_seq = Viterbi(words)
    self.toutput.text = 'Output:'+str(tagged_seq)

    MainApp().run()
